{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d47d29",
   "metadata": {},
   "source": [
    "# Deep Learning Aplicado a la Detección Automática de Valvulopatías Usando Sonidos Cardíacos\n",
    "John Jairo Gelpud Chachinoy<sup>1</sup>, Silvia Gabriela Castillo Pastuzan<sup>2</sup>\n",
    "\n",
    "Director: PhD. Wilson O. Achicanoy Martínez<sup>3</sup>\n",
    "\n",
    "Coasesor: PhD(c). Mario Fernando Jojoa<sup>4</sup>\n",
    "\n",
    "<sup>1</sup>Departamento de Electrónica, Universidad de Nariño, Pasto, e-mail: johnjairog@udenar.edu.co\n",
    "\n",
    "<sup>2</sup>Departamento de Electrónica, Universidad de Nariño, Pasto, e-mail:gabrielacast@udenar.edu.co\n",
    "\n",
    "<sup>3</sup>Departamento de Electrónica, Universidad de Nariño, Pasto, e-mail:wilachic@udenar.edu.co\n",
    "\n",
    "<sup>4</sup>Universidad de Deusto, España, e-mail:mariojojoa@deusto.es\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8abae",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "El presente código hace parte de los documentos desarrollados como parte del trabajo de tesis de grado para optar por el título de ingeniero electrónico en la Universidad de Nariño. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b9870",
   "metadata": {},
   "source": [
    "## Código de Inicialización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d858b3",
   "metadata": {},
   "source": [
    "### Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "import scikitplot as skplt\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams.update({'font.size':18})\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a27d675",
   "metadata": {},
   "source": [
    "### Cargar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se está ejecutando en Google Colab definir colab=True\n",
    "colab = False\n",
    "# Si se está ejecutando localmente especificar el directorio en dataset_path\n",
    "dataset_path = 'D:/UDENAR/Electronic Engineering/Trabajo de Grado/Results/Teager/Classification/CardioDeep2/Datasets/'\n",
    "features_path = ''\n",
    "\n",
    "if colab:\n",
    "    dataset_path = 'Datasets/'\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    data = zipfile.ZipFile(io.BytesIO(uploaded['Datasets.zip']), 'r')\n",
    "    data.extractall()\n",
    "else:\n",
    "    print('Ejecutando en sistema local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de las imágenes para la etapa de entrenamiento y validación\n",
    "data_transforms = {\n",
    "    'train':transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val':transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test':transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad02213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea los datasets y dataloaders con un batch de 4 imágenes, y aplica las transformaciones\n",
    "image_datasets = {x: datasets.ImageFolder(dataset_path+x, data_transforms[x])\n",
    "                      for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1, shuffle=False,\n",
    "                                              num_workers=2) for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes # nombres de las categorias\n",
    "# Selecciona la GPU o CPU como dispositivo para realizar las operaciones y\n",
    "# almacenar variables\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd5f1a",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e896cd0",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad740901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(model, dataloaders, dataset_sizes, num_ftrs):\n",
    "    train_ftrs = torch.empty((dataset_sizes['train'], num_ftrs+1), device=device)\n",
    "    val_ftrs = torch.empty((dataset_sizes['val'], num_ftrs+1), device=device)\n",
    "    test_ftrs = torch.empty((dataset_sizes['test'], num_ftrs+1), device=device)\n",
    "    \n",
    "    for dataset, array in zip(['train', 'val', 'test'], [train_ftrs, val_ftrs, test_ftrs]):\n",
    "        \n",
    "        for i, data in zip(range(dataset_sizes[dataset]), dataloaders[dataset]):\n",
    "            inputs = data[0]\n",
    "            labels = data[1]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            array[i, :-1] = outputs.squeeze()\n",
    "            array[i, -1] = labels\n",
    "    \n",
    "    train_ftrs = train_ftrs.cpu().detach().numpy()\n",
    "    val_ftrs = val_ftrs.cpu().detach().numpy()\n",
    "    test_ftrs = test_ftrs.cpu().detach().numpy()\n",
    "    \n",
    "    return train_ftrs, val_ftrs, test_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(train_ftrs, val_ftrs, test_ftrs, model, path):\n",
    "    columns = ['ftr_'+str(x) for x in range(train_ftrs.shape[1]-1)]\n",
    "    columns.append('label')\n",
    "    \n",
    "    train_dataset = pd.DataFrame(train_ftrs, columns=columns)\n",
    "    val_dataset = pd.DataFrame(val_ftrs, columns=columns)\n",
    "    test_dataset = pd.DataFrame(test_ftrs, columns=columns)\n",
    "    \n",
    "    for phase, dataframe in zip(['train', 'val', 'test'], [train_dataset, val_dataset, test_dataset]):\n",
    "        dataframe.to_excel(path+model+'/'+phase+'.xlsx')\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426fb9e",
   "metadata": {},
   "source": [
    "### ConvNets as features extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c21ffc",
   "metadata": {},
   "source": [
    "#### ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = models.resnet152(pretrained=True)\n",
    "res_num_ftrs = res_model.fc.in_features\n",
    "\n",
    "modules = list(res_model.children())[:-1]\n",
    "fe_res_model = nn.Sequential(*modules)\n",
    "\n",
    "for param in res_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "fe_res_model.eval()\n",
    "fe_res_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26459d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train_ftrs, res_val_ftrs, res_test_ftrs = feature_extraction(fe_res_model, dataloaders,\n",
    "                                                                 dataset_sizes, res_num_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train_df, res_val_df, res_test_df = save_features(res_train_ftrs, res_val_ftrs,\n",
    "                                                      res_test_ftrs, 'ResNet152',\n",
    "                                                      features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4103910a",
   "metadata": {},
   "source": [
    "#### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = models.vgg16(pretrained=True)\n",
    "vgg_num_ftrs = list(vgg_model.classifier.children())[-1].in_features\n",
    "\n",
    "fe_vgg_model = vgg_model\n",
    "fe_vgg_model.classifier = fe_vgg_model.classifier[:-1]\n",
    "\n",
    "for param in fe_vgg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "fe_vgg_model.eval()\n",
    "fe_vgg_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_train_ftrs, vgg_val_ftrs, vgg_test_ftrs = feature_extraction(fe_vgg_model, dataloaders,\n",
    "                                                                 dataset_sizes, vgg_num_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_train_df, vgg_val_df, vgg_test_df = save_features(vgg_train_ftrs, vgg_val_ftrs,\n",
    "                                                      vgg_test_ftrs, 'VGG16',\n",
    "                                                      features_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
